{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "machine_shape": "hm",
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This notebook includes the use of five different classification models (VGG16, ResNet50, Inception V3, Densenet121, 4 Layer CNN) to distinguish between Alzheimer's and healthy controls using scalogram images from all lobes. The models were trained and evaluated using all the epochs (segments) of the images.\n",
    "\n",
    "*Fatemeh Makhloughi*"
   ],
   "metadata": {
    "id": "paFR_tc9K19P"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QmGL1r444dDF"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'OrderedDict' from 'typing' (C:\\Users\\BioElectric\\AppData\\Local\\Programs\\Python\\Python37\\lib\\typing.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_15248\\3307492257.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmetrics\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mconfusion_matrix\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mclassification_report\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maccuracy_score\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mprecision_score\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrecall_score\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf1_score\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel_selection\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtrain_test_split\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mKFold\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpreprocessing\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimage\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mload_img\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mimg_to_array\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutils\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mto_categorical\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodels\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mSequential\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mModel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mclone_model\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mload_model\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\Abdipour\\Alzheimer_1Hz\\venv\\lib\\site-packages\\tensorflow\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtyping\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0m_typing\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 37\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtools\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmodule_util\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0m_module_util\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     38\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutil\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlazy_loader\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mLazyLoader\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0m_LazyLoader\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\Abdipour\\Alzheimer_1Hz\\venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     40\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[1;31m# Bring in subpackages.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 42\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     43\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdistribute\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[1;31m# from tensorflow.python import keras\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\Abdipour\\Alzheimer_1Hz\\venv\\lib\\site-packages\\tensorflow\\python\\data\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[1;31m# pylint: disable=unused-import\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 21\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mexperimental\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     22\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset_ops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mAUTOTUNE\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset_ops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mDataset\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\Abdipour\\Alzheimer_1Hz\\venv\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     94\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     95\u001B[0m \u001B[1;31m# pylint: disable=unused-import\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 96\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mservice\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     97\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbatching\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdense_to_ragged_batch\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     98\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbatching\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdense_to_sparse_batch\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\Abdipour\\Alzheimer_1Hz\\venv\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    417\u001B[0m \"\"\"\n\u001B[0;32m    418\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 419\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata_service_ops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdistribute\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    420\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata_service_ops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mfrom_dataset_id\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    421\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata_service_ops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mregister_dataset\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\Abdipour\\Alzheimer_1Hz\\venv\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mservice\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0m_pywrap_server_lib\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mservice\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0m_pywrap_utils\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 25\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdataset_ops\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     26\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0moptions\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0moptions_lib\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mstructured_function\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\Abdipour\\Alzheimer_1Hz\\venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mgraph_pb2\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtf2\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 29\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0miterator_ops\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     30\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0moptions\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0moptions_lib\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mstructured_function\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\Abdipour\\Alzheimer_1Hz\\venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mgen_dataset_ops\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrackable\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mbase\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mtrackable\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 34\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtraining\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msaver\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mBaseSaverBuilder\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     35\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutil\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0m_pywrap_utils\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutil\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdeprecation\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\Abdipour\\Alzheimer_1Hz\\venv\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprotobuf\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0msaver_pb2\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprotobuf\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtrackable_object_graph_pb2\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 32\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcheckpoint\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mcheckpoint_management\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     33\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclient\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0msession\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meager\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mcontext\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\Abdipour\\Alzheimer_1Hz\\venv\\lib\\site-packages\\tensorflow\\python\\checkpoint\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;34m\"\"\"API defining checkpoint.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcheckpoint\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mcheckpoint_view\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mF:\\Abdipour\\Alzheimer_1Hz\\venv\\lib\\site-packages\\tensorflow\\python\\checkpoint\\checkpoint_view.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprotobuf\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtrackable_object_graph_pb2\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcheckpoint\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtrackable_view\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     20\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0merrors_impl\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mplatform\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtf_logging\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mlogging\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\Abdipour\\Alzheimer_1Hz\\venv\\lib\\site-packages\\tensorflow\\python\\checkpoint\\trackable_view.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrackable\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mbase\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 20\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrackable\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mconverter\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     21\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutil\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mobject_identity\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutil\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtf_export\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtf_export\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\Abdipour\\Alzheimer_1Hz\\venv\\lib\\site-packages\\tensorflow\\python\\trackable\\converter.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 18\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meager\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolymorphic_function\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0msaved_model_utils\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     19\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdtypes\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtensor_util\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\Abdipour\\Alzheimer_1Hz\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\saved_model_utils.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrackable\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0masset\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrackable\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mbase\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mtrackable\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 36\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrackable\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mresource\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     37\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\Abdipour\\Alzheimer_1Hz\\venv\\lib\\site-packages\\tensorflow\\python\\trackable\\resource.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meager\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mcontext\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 22\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meager\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdef_function\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     23\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrackable\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mbase\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\Abdipour\\Alzheimer_1Hz\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[1;31m# Config Options\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 20\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meager\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolymorphic_function\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolymorphic_function\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mset_dynamic_variable_creation\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     21\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meager\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolymorphic_function\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolymorphic_function\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mrun_functions_eagerly\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meager\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolymorphic_function\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolymorphic_function\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mfunctions_run_eagerly\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\Abdipour\\Alzheimer_1Hz\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     74\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meager\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mlift_to_graph\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     75\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meager\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmonitoring\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 76\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meager\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolymorphic_function\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mfunction_spec\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mfunction_spec_lib\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     77\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meager\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolymorphic_function\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmonomorphic_function\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     78\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meager\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolymorphic_function\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtracing_compiler\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\Abdipour\\Alzheimer_1Hz\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\function_spec.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfunction\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtrace_type\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 25\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfunction\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolymorphism\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mfunction_type\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mfunction_type_lib\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     26\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meager\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolymorphic_function\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mcomposite_tensor_utils\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mcomposite_tensor\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\Abdipour\\Alzheimer_1Hz\\venv\\lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\function_type.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mcollections\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0minspect\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtyping\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mAny\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mCallable\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mDict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mMapping\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mOptional\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mSequence\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mTuple\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mOrderedDict\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     20\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfunction\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtrace_type\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'OrderedDict' from 'typing' (C:\\Users\\BioElectric\\AppData\\Local\\Programs\\Python\\Python37\\lib\\typing.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.python.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "from tensorflow.python.keras.models import Sequential, Model, clone_model, load_model\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.layers import Conv2D, Input, ConvLSTM2D, MaxPooling3D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation, GlobalAveragePooling2D, MaxPool2D, LSTM, GRU, TimeDistributed\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.python.keras.regularizers import l2\n",
    "from tensorflow.python.keras.applications import VGG16, VGG19, ResNet50, InceptionV3, DenseNet121, EfficientNetB2\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z9DE0DZG4hPB",
    "outputId": "5a011522-85f1-4c8b-88cc-c10c98ce795a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### In local computer"
   ],
   "metadata": {
    "id": "14zCxr8ppTy6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Define the local directory path\n",
    "data_dir = os.path.join('F:', 'Abdipour', 'Alzheimer', 'Spectrogram Images', 'Temporal')\n",
    "\n",
    "# Function to extract the numeric part of the folder name\n",
    "def extract_numeric_part(folder_name):\n",
    "    # Use a regular expression to find the numeric part at the end of the folder name\n",
    "    match = re.search(r'\\d+$', folder_name)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    else:\n",
    "        raise ValueError(f\"No numeric part found in folder name: {folder_name}\")\n",
    "\n",
    "# List all directories in the specified directory and sort them based on the numeric part\n",
    "folders = sorted([f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))], key=extract_numeric_part)\n",
    "\n",
    "subject_folders, labels = [], []\n",
    "\n",
    "# Iterate through the sorted folders\n",
    "for folder_name in folders:\n",
    "    # Extract the numeric part from the folder name\n",
    "    subject_num = extract_numeric_part(folder_name)\n",
    "    subject_folder = os.path.join(data_dir, folder_name)\n",
    "    subject_folders.append(subject_folder)\n",
    "\n",
    "    # Assign labels based on the numeric part\n",
    "    if subject_num <= 36:\n",
    "        labels.append(0)  # AD group\n",
    "    elif 37 <= subject_num <= 65:\n",
    "        labels.append(1)  # HC group\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "subject_folders = np.array(subject_folders)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Print the results (optional)\n",
    "print(\"Subject Folders:\", subject_folders)\n",
    "print(\"Labels:\", labels)\n"
   ],
   "metadata": {
    "id": "vQmXNRBWpTcC"
   },
   "execution_count": 2,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'F:Abdipour\\\\Alzheimer\\\\Spectrogram Images\\\\Temporal'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_9360\\3683039572.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;31m# List all directories in the specified directory and sort them based on the numeric part\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 18\u001B[1;33m \u001B[0mfolders\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msorted\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mf\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mf\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlistdir\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata_dir\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misdir\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata_dir\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mextract_numeric_part\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     19\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[0msubject_folders\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [WinError 3] The system cannot find the path specified: 'F:Abdipour\\\\Alzheimer\\\\Spectrogram Images\\\\Temporal'"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1Hz- Temporal Lobe- Ignore Concatenate"
   ],
   "metadata": {
    "id": "aIPsAkSqnTab"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Define the directory path\n",
    "data_dir = os.path.join('/content/drive/My Drive/', 'Alzheimer', 'Spectrogram Images', 'Temporal')\n",
    "\n",
    "# Function to extract the numeric part of the folder name\n",
    "def extract_numeric_part(folder_name):\n",
    "    # Use a regular expression to find the numeric part at the end of the folder name\n",
    "    match = re.search(r'\\d+$', folder_name)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    else:\n",
    "        raise ValueError(f\"No numeric part found in folder name: {folder_name}\")\n",
    "\n",
    "# List all directories in the specified directory and sort them based on the numeric part\n",
    "folders = sorted([f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))], key=extract_numeric_part)\n",
    "\n",
    "subject_folders, labels = [], []\n",
    "\n",
    "# Iterate through the sorted folders\n",
    "for folder_name in folders:\n",
    "    # Extract the numeric part from the folder name\n",
    "    subject_num = extract_numeric_part(folder_name)\n",
    "    subject_folder = os.path.join(data_dir, folder_name)\n",
    "    subject_folders.append(subject_folder)\n",
    "\n",
    "    # Assign labels based on the numeric part\n",
    "    if subject_num <= 36:\n",
    "        labels.append(0)  # AD group\n",
    "    elif 37 <= subject_num <= 65:\n",
    "        labels.append(1)  # HC group\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "subject_folders = np.array(subject_folders)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Print the results (optional)\n",
    "print(\"Subject Folders:\", subject_folders)\n",
    "print(\"Labels:\", labels)\n"
   ],
   "metadata": {
    "id": "fm00YzZtdF-v",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "43efcb7d-c7e1-4a08-a83b-da348ce00af6"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Subject Folders: ['/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject1'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject2'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject3'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject4'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject5'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject6'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject7'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject8'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject9'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject10'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject11'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject12'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject13'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject14'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject15'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject16'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject17'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject18'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject19'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject20'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject22'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject23'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject24'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject25'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject26'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject27'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject28'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject29'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject30'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject31'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject32'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject33'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject34'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject35'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject36'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject37'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject38'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject39'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject40'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject41'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject42'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject43'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject44'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject45'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject46'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject47'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject48'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject49'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject50'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject51'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject52'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject53'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject54'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject55'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject56'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject57'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject58'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject59'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject60'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject61'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject62'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject63'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject64'\n",
      " '/content/drive/My Drive/Alzheimer/Spectrogram Images/Temporal/subject65']\n",
      "Labels: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def load_images_and_labels(subjects, labels):\n",
    "    images, labels_t = [], []\n",
    "    for subject_folder, label in zip(subjects, labels):\n",
    "        for filename in os.listdir(subject_folder):\n",
    "                img_path = os.path.join(subject_folder, filename.decode())\n",
    "                try:\n",
    "                    img = load_img(img_path, target_size=(224, 224))\n",
    "                    img_array = img_to_array(img)\n",
    "                    img_array = img_array / 255.0\n",
    "                    images.append(img_array)\n",
    "                    labels_t.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image: {img_path}, {e}\")\n",
    "\n",
    "    return np.array(images), np.array(labels_t)"
   ],
   "metadata": {
    "id": "ktvVT0n9nedj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def load_and_preprocess_data(subject_folders, labels):\n",
    "    train_subjects, test_subjects, train_labels, test_labels = train_test_split(subject_folders, labels, test_size=0.2, stratify=labels, random_state = 42)\n",
    "\n",
    "    train_subjects, val_subjects, train_labels, val_labels = train_test_split(train_subjects, train_labels, test_size=0.1, stratify=train_labels, random_state = 42)\n",
    "\n",
    "    train_images, train_labels_t = load_images_and_labels(train_subjects, train_labels)\n",
    "    val_images, val_labels_t = load_images_and_labels(val_subjects, val_labels)\n",
    "\n",
    "    test_image_counts_per_subject = [len(os.listdir(folder)) for folder in test_subjects]\n",
    "\n",
    "    test_images, test_labels_t = load_images_and_labels(test_subjects, test_labels)\n",
    "\n",
    "    return train_images, train_labels_t, val_images, val_labels_t, test_images, test_labels_t, test_subjects, test_image_counts_per_subject\n"
   ],
   "metadata": {
    "id": "opXdIhf_niq-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_new_model(base_model):\n",
    "    model = clone_model(base_model)\n",
    "\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(256)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=model.input, outputs=predictions)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.01), metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "id": "ZAJ11C0VnjZb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_and_evaluate_model(base_model, subject_folders, labels, num_epochs=50, num_iterations=5, model_name='model'):\n",
    "    accuracy_list, precision_list, recall_list, f1_list = [], [], [], []\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"\\nIteration {iteration + 1}:\")\n",
    "\n",
    "        model = create_new_model(base_model)\n",
    "\n",
    "        train_images, train_labels_t, val_images, val_labels_t, test_images, test_labels_t, test_subjects, test_image_counts_per_subject = load_and_preprocess_data(subject_folders, labels)\n",
    "\n",
    "        train_data = list(zip(train_images, train_labels_t))\n",
    "        np.random.shuffle(train_data)\n",
    "        train_images, train_labels_t = zip(*train_data)\n",
    "\n",
    "        val_data = list(zip(val_images, val_labels_t))\n",
    "        np.random.shuffle(val_data)\n",
    "        val_images, val_labels_t = zip(*val_data)\n",
    "\n",
    "        # test_data = list(zip(test_images, test_labels_t))\n",
    "        # np.random.shuffle(test_data)\n",
    "        # test_images, test_labels_t = zip(*test_data)\n",
    "\n",
    "        train_images, train_labels_t = np.array(train_images), np.array(train_labels_t)\n",
    "        val_images, val_labels_t = np.array(val_images), np.array(val_labels_t)\n",
    "        # test_images, test_labels_t = np.array(test_images), np.array(test_labels_t)\n",
    "\n",
    "\n",
    "\n",
    "        # checkpoint_filepath = \"best_weights_iteration_{iteration + 1}.h5\"\n",
    "        # checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "        #                                        monitor='val_accuracy',\n",
    "        #                                        mode='max',\n",
    "        #                                        save_best_only=True,\n",
    "        #                                        save_weights_only=True,\n",
    "        #                                        verbose=1)\n",
    "\n",
    "        # history = model.fit(train_images, train_labels_t, epochs=num_epochs, batch_size=8, validation_data=(val_images, val_labels_t), callbacks=[checkpoint], verbose=1)\n",
    "\n",
    "        checkpoint_dir = '/content'\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "        local_checkpoint_filepath = os.path.join(checkpoint_dir, f\"{model_name}_best_weights_iteration_{iteration + 1}.h5\")\n",
    "        drive_checkpoint_filepath = f'/content/drive/My Drive/{model_name}_best_weights_iteration_{iteration + 1}.h5'\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(filepath=local_checkpoint_filepath,\n",
    "                                               monitor='val_accuracy',\n",
    "                                               mode='max',\n",
    "                                               save_best_only=True,\n",
    "                                               save_weights_only=True,\n",
    "                                               verbose=1)\n",
    "\n",
    "        early_stopping_callback = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "        history = model.fit(train_images, train_labels_t, epochs=num_epochs, batch_size=32, validation_data=(val_images, val_labels_t), callbacks=[checkpoint_callback, early_stopping_callback], verbose=1)\n",
    "\n",
    "        if os.path.exists(local_checkpoint_filepath):\n",
    "            print(f\"Best weights saved locally at: {local_checkpoint_filepath}\")\n",
    "\n",
    "            shutil.copy(local_checkpoint_filepath, drive_checkpoint_filepath)\n",
    "            if os.path.exists(drive_checkpoint_filepath):\n",
    "                print(f\"Best weights also copied to Google Drive at: {drive_checkpoint_filepath}\")\n",
    "            else:\n",
    "                print(f\"Failed to copy best weights to Google Drive at: {drive_checkpoint_filepath}\")\n",
    "        else:\n",
    "            print(f\"Failed to save best weights locally at: {local_checkpoint_filepath}\")\n",
    "\n",
    "        model.load_weights(local_checkpoint_filepath)\n",
    "\n",
    "        test_predictions = model.predict(test_images)\n",
    "        test_predictions = (test_predictions > 0.5).astype(int)\n",
    "\n",
    "        accuracy = accuracy_score(test_labels_t, test_predictions)\n",
    "        precision = precision_score(test_labels_t, test_predictions)\n",
    "        recall = recall_score(test_labels_t, test_predictions)\n",
    "        f1 = f1_score(test_labels_t, test_predictions)\n",
    "\n",
    "        accuracy_list.append(accuracy)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "        print(\"ModelCheckpoint callback triggered and saved the best weights.\")\n",
    "\n",
    "\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "        plt.title('Model accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        file_name = f'{model_name}_accuracy_iteration_{iteration + 1}.png'\n",
    "        plt.savefig(file_name)\n",
    "        plt.close()\n",
    "        files.download(file_name)\n",
    "\n",
    "\n",
    "\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Model loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        file_name = f'{model_name}_loss_iteration_{iteration + 1}.png'\n",
    "        plt.savefig(file_name)\n",
    "        plt.close()\n",
    "        files.download(file_name)\n",
    "\n",
    "\n",
    "        cm = confusion_matrix(test_labels_t, test_predictions)\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', xticklabels=['AD', 'HC'], yticklabels=['AD', 'HC'])\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title(f'Confusion Matrix - {model_name} - Iteration {iteration + 1}')\n",
    "        file_name = f'{model_name}_confusion_matrix_iteration_{iteration + 1}.png'\n",
    "        plt.savefig(file_name)\n",
    "        plt.close()\n",
    "        files.download(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nPredictions for Each Subject's Images:\")\n",
    "        # start_idx = 0\n",
    "        # for i, (subject_folder, num_images) in enumerate(zip(test_subjects, test_image_counts_per_subject)):\n",
    "        #     end_idx = start_idx + num_images\n",
    "        #     subject_images = test_images[start_idx:end_idx]\n",
    "        #     subject_predictions = (model.predict(subject_images) > 0.5).astype(int)\n",
    "        #     print(f\"Subject {i + 1} ({subject_folder}):\")\n",
    "        #     for j, prediction in enumerate(subject_predictions):\n",
    "        #         if prediction == 0:\n",
    "        #             print(f\"Image {j + 1}: AD\")\n",
    "        #         else:\n",
    "        #             print(f\"Image {j + 1}: HC\")\n",
    "        #     start_idx = end_idx\n",
    "\n",
    "\n",
    "        print(\"\\nPredictions for Each Subject's Images:\")\n",
    "        start_idx = 0\n",
    "        for i, (subject_folder, num_images) in enumerate(zip(test_subjects, test_image_counts_per_subject)):\n",
    "            end_idx = start_idx + num_images\n",
    "            subject_images = test_images[start_idx:end_idx]\n",
    "            subject_predictions = (model.predict(subject_images) > 0.5).astype(int)\n",
    "            ad_count = sum(subject_predictions == 0)\n",
    "            hc_count = sum(subject_predictions == 1)\n",
    "            print(f\"Subject {i + 1} ({subject_folder}):\")\n",
    "            print(f\"AD count: {ad_count}, HC count: {hc_count}\")\n",
    "            start_idx = end_idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\nAverage Metrics:\")\n",
    "    print(f\"Average Accuracy: {np.mean(accuracy_list):.4f}\")\n",
    "    print(f\"Average Precision: {np.mean(precision_list):.4f}\")\n",
    "    print(f\"Average Recall: {np.mean(recall_list):.4f}\")\n",
    "    print(f\"Average F1 Score: {np.mean(f1_list):.4f}\")\n"
   ],
   "metadata": {
    "id": "rUC7gSytnlkO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63x4Sc0NUKdy"
   },
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "vgg16_base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AcwK5Vd7pDzR",
    "outputId": "eadb862c-db34-4cfa-a87d-71e2ada15ef9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 0s 0us/step\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "    # x = model.output\n",
    "    # x = Flatten()(x)\n",
    "    # x = Dense(128)(x)\n",
    "    # x = Dropout(0.2)(x)\n",
    "    # x = Dense(256)(x)\n",
    "    # x = Dropout(0.2)(x)\n",
    "model_name = 'vgg16_1Hz_Temporal'\n",
    "train_and_evaluate_model(vgg16_base_model, subject_folders, labels, num_epochs=100, num_iterations=5, model_name=model_name)"
   ],
   "metadata": {
    "id": "djsR2B8-HxZD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ResNet50"
   ],
   "metadata": {
    "id": "0YpkoYLeAteM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ResNet50_base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6GIK8gkAw3G",
    "outputId": "6df2e6f3-6746-4caa-cfdd-e7413ab034b4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 0s 0us/step\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model_name = 'ResNet50_Temporal'\n",
    "train_and_evaluate_model(ResNet50_base_model, subject_folders, labels, num_epochs=100, num_iterations=5, model_name=model_name)"
   ],
   "metadata": {
    "id": "LNxuiYQUA2XR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Densenet121"
   ],
   "metadata": {
    "id": "YkwZVdiODPRM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "Dense_base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_BhNKs9DO04",
    "outputId": "c2c34b85-e45d-44a2-c0fa-989872c8b652"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "29084464/29084464 [==============================] - 0s 0us/step\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model_name = 'Densenet121_Temporal'\n",
    "train_and_evaluate_model(Dense_base_model, subject_folders, labels, num_epochs=100, num_iterations=5, model_name=model_name)"
   ],
   "metadata": {
    "id": "YjQFZP8FDcQ4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inception V3"
   ],
   "metadata": {
    "id": "9AhUFR-M5yV-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "Inception_base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))"
   ],
   "metadata": {
    "id": "pd5jxEE65yV_",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "82d89180-dede-447d-e448-671e662ed8d7"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87910968/87910968 [==============================] - 0s 0us/step\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model_name = 'InceptionV3_Temporal'\n",
    "train_and_evaluate_model(Inception_base_model, subject_folders, labels, num_epochs=100, num_iterations=5, model_name=model_name)"
   ],
   "metadata": {
    "id": "3OkgnXPa5yWA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bziYzKNiaPmT"
   },
   "source": [
    "## 4 layer CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0XqbFTmJaPmU"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_new_model(input_shape):\n",
    "    base_model = Input(shape=input_shape)\n",
    "    x = Conv2D(32, (3, 3), activation='relu')(base_model)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = Conv2D(256, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=base_model, outputs=predictions)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.01), metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhQcJoUcaPmV"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(subject_folders, labels, num_epochs=50, num_iterations=5, model_name='model'):\n",
    "    accuracy_list, precision_list, recall_list, f1_list = [], [], [], []\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"\\nIteration {iteration + 1}:\")\n",
    "        input_shape = (224, 224, 3)\n",
    "\n",
    "        model = create_new_model(input_shape)\n",
    "\n",
    "        train_images, train_labels_t, val_images, val_labels_t, test_images, test_labels_t, test_subjects, test_image_counts_per_subject = load_and_preprocess_data(subject_folders, labels)\n",
    "\n",
    "        train_data = list(zip(train_images, train_labels_t))\n",
    "        np.random.shuffle(train_data)\n",
    "        train_images, train_labels_t = zip(*train_data)\n",
    "\n",
    "        val_data = list(zip(val_images, val_labels_t))\n",
    "        np.random.shuffle(val_data)\n",
    "        val_images, val_labels_t = zip(*val_data)\n",
    "\n",
    "        # test_data = list(zip(test_images, test_labels_t))\n",
    "        # np.random.shuffle(test_data)\n",
    "        # test_images, test_labels_t = zip(*test_data)\n",
    "\n",
    "        train_images, train_labels_t = np.array(train_images), np.array(train_labels_t)\n",
    "        val_images, val_labels_t = np.array(val_images), np.array(val_labels_t)\n",
    "        # test_images, test_labels_t = np.array(test_images), np.array(test_labels_t)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # checkpoint_filepath = \"best_weights_iteration_{iteration + 1}.h5\"\n",
    "        # checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "        #                                        monitor='val_accuracy',\n",
    "        #                                        mode='max',\n",
    "        #                                        save_best_only=True,\n",
    "        #                                        save_weights_only=True,\n",
    "        #                                        verbose=1)\n",
    "\n",
    "        # history = model.fit(train_images, train_labels_t, epochs=num_epochs, batch_size=8, validation_data=(val_images, val_labels_t), callbacks=[checkpoint], verbose=1)\n",
    "\n",
    "\n",
    "        checkpoint_dir = '/content'\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "        local_checkpoint_filepath = os.path.join(checkpoint_dir, f\"{model_name}_best_weights_iteration_{iteration + 1}.h5\")\n",
    "        drive_checkpoint_filepath = f'/content/drive/My Drive/{model_name}_best_weights_iteration_{iteration + 1}.h5'\n",
    "\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(filepath=local_checkpoint_filepath,\n",
    "                                               monitor='val_accuracy',\n",
    "                                               mode='max',\n",
    "                                               save_best_only=True,\n",
    "                                               save_weights_only=True,\n",
    "                                               verbose=1)\n",
    "\n",
    "\n",
    "        early_stopping_callback = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "        history = model.fit(train_images, train_labels_t, epochs=num_epochs, batch_size=8, validation_data=(val_images, val_labels_t), callbacks=[checkpoint_callback, early_stopping_callback], verbose=1)\n",
    "\n",
    "\n",
    "        if os.path.exists(local_checkpoint_filepath):\n",
    "            print(f\"Best weights saved locally at: {local_checkpoint_filepath}\")\n",
    "\n",
    "\n",
    "            shutil.copy(local_checkpoint_filepath, drive_checkpoint_filepath)\n",
    "            if os.path.exists(drive_checkpoint_filepath):\n",
    "                print(f\"Best weights also copied to Google Drive at: {drive_checkpoint_filepath}\")\n",
    "            else:\n",
    "                print(f\"Failed to copy best weights to Google Drive at: {drive_checkpoint_filepath}\")\n",
    "        else:\n",
    "            print(f\"Failed to save best weights locally at: {local_checkpoint_filepath}\")\n",
    "\n",
    "\n",
    "        model.load_weights(local_checkpoint_filepath)\n",
    "\n",
    "\n",
    "        test_predictions = model.predict(test_images)\n",
    "        test_predictions = (test_predictions > 0.5).astype(int)\n",
    "\n",
    "        accuracy = accuracy_score(test_labels_t, test_predictions)\n",
    "        precision = precision_score(test_labels_t, test_predictions)\n",
    "        recall = recall_score(test_labels_t, test_predictions)\n",
    "        f1 = f1_score(test_labels_t, test_predictions)\n",
    "\n",
    "        accuracy_list.append(accuracy)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "\n",
    "        print(\"ModelCheckpoint callback triggered and saved the best weights.\")\n",
    "\n",
    "\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "        plt.title('Model accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        file_name = f'{model_name}_accuracy_iteration_{iteration + 1}.png'\n",
    "        plt.savefig(file_name)\n",
    "        plt.close()\n",
    "        files.download(file_name)\n",
    "\n",
    "\n",
    "\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Model loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        file_name = f'{model_name}_loss_iteration_{iteration + 1}.png'\n",
    "        plt.savefig(file_name)\n",
    "        plt.close()\n",
    "        files.download(file_name)\n",
    "\n",
    "\n",
    "        cm = confusion_matrix(test_labels_t, test_predictions)\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', xticklabels=['AD', 'HC'], yticklabels=['AD', 'HC'])\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title(f'Confusion Matrix - {model_name} - Iteration {iteration + 1}')\n",
    "        file_name = f'{model_name}_confusion_matrix_iteration_{iteration + 1}.png'\n",
    "        plt.savefig(file_name)\n",
    "        plt.close()\n",
    "        files.download(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nPredictions for Each Subject's Images:\")\n",
    "        # start_idx = 0\n",
    "        # for i, (subject_folder, num_images) in enumerate(zip(test_subjects, test_image_counts_per_subject)):\n",
    "        #     end_idx = start_idx + num_images\n",
    "        #     subject_images = test_images[start_idx:end_idx]\n",
    "        #     subject_predictions = (model.predict(subject_images) > 0.5).astype(int)\n",
    "        #     print(f\"Subject {i + 1} ({subject_folder}):\")\n",
    "        #     for j, prediction in enumerate(subject_predictions):\n",
    "        #         if prediction == 0:\n",
    "        #             print(f\"Image {j + 1}: AD\")\n",
    "        #         else:\n",
    "        #             print(f\"Image {j + 1}: HC\")\n",
    "        #     start_idx = end_idx\n",
    "\n",
    "\n",
    "        print(\"\\nPredictions for Each Subject's Images:\")\n",
    "        start_idx = 0\n",
    "        for i, (subject_folder, num_images) in enumerate(zip(test_subjects, test_image_counts_per_subject)):\n",
    "            end_idx = start_idx + num_images\n",
    "            subject_images = test_images[start_idx:end_idx]\n",
    "            subject_predictions = (model.predict(subject_images) > 0.5).astype(int)\n",
    "            ad_count = sum(subject_predictions == 0)\n",
    "            hc_count = sum(subject_predictions == 1)\n",
    "            print(f\"Subject {i + 1} ({subject_folder}):\")\n",
    "            print(f\"AD count: {ad_count}, HC count: {hc_count}\")\n",
    "            start_idx = end_idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\nAverage Metrics:\")\n",
    "    print(f\"Average Accuracy: {np.mean(accuracy_list):.4f}\")\n",
    "    print(f\"Average Precision: {np.mean(precision_list):.4f}\")\n",
    "    print(f\"Average Recall: {np.mean(recall_list):.4f}\")\n",
    "    print(f\"Average F1 Score: {np.mean(f1_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VqavgxtVaPmV"
   },
   "outputs": [],
   "source": [
    "train_and_evaluate_model(subject_folders, labels, num_epochs=100, num_iterations=5, model_name='4LCNN_Temporal')"
   ]
  }
 ]
}
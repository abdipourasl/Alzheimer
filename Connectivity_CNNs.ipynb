{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uH5jN3EHXjs1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import Counter\n",
        "from PIL import Image\n",
        "# from google.colab import files \n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.models import Sequential, Model, clone_model      \n",
        "from keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Conv2D, Input, ConvLSTM2D,MaxPooling3D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation, GlobalAveragePooling2D, MaxPool2D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback, LearningRateScheduler\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.applications import VGG16, VGG19, ResNet50, InceptionV3, DenseNet121, EfficientNetB2\n",
        "from tensorflow.keras.layers import LSTM, GRU, TimeDistributed, Input\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jrZfB_hgxav"
      },
      "source": [
        "#  Connectivity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "sbKw69HjYufV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Subject Folders: ['F:/Abdipour/Connectivity/Pearson images\\\\Subject1'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject10'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject11'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject12'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject13'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject14'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject15'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject16'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject17'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject18'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject19'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject2'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject20'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject21'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject22'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject23'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject24'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject25'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject26'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject27'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject28'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject29'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject3'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject30'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject31'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject32'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject33'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject34'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject35'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject36'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject37'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject38'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject39'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject4'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject40'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject41'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject42'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject43'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject44'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject45'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject46'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject47'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject48'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject49'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject5'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject50'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject51'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject52'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject53'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject54'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject55'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject56'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject57'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject58'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject59'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject6'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject60'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject61'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject62'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject63'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject64'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject65'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject7'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject8'\n",
            " 'F:/Abdipour/Connectivity/Pearson images\\\\Subject9']\n",
            "Labels: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "data_dir = 'F:/Abdipour/Connectivity/Pearson images'\n",
        "\n",
        "def extract_numeric_part(folder_name):\n",
        "    parts = folder_name.split('_')\n",
        "    for part in parts:\n",
        "        if part.startswith('subject'):\n",
        "            return int(part.replace('subject', ''))\n",
        "    return -1  # Return -1 if no numeric part is found\n",
        "\n",
        "folders = sorted([f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))], key=extract_numeric_part)\n",
        "\n",
        "subject_folders, labels = [], []\n",
        "\n",
        "for folder_name in folders:\n",
        "    subject_num = extract_numeric_part(folder_name)\n",
        "    subject_folder = os.path.join(data_dir, folder_name)\n",
        "    subject_folders.append(subject_folder)\n",
        "\n",
        "    if subject_num <= 36:\n",
        "        labels.append(0)  # AD group\n",
        "    elif 37 <= subject_num <= 65:\n",
        "        labels.append(1)  # HC group\n",
        "\n",
        "subject_folders = np.array(subject_folders)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(\"Subject Folders:\", subject_folders)\n",
        "print(\"Labels:\", labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_images_and_labels(subjects, labels):\n",
        "    images, labels_t = [], []\n",
        "    for subject_folder, label in zip(subjects, labels):\n",
        "        for filename in os.listdir(subject_folder):\n",
        "            img_path = os.path.join(subject_folder, filename)\n",
        "            try:\n",
        "                img = Image.open(img_path)\n",
        "                img = img.resize((224, 224))\n",
        "                img_array = np.array(img, dtype=np.float32)\n",
        "                img_array = img_array / 255.0  \n",
        "                images.append(img_array)\n",
        "                labels_t.append(label)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image: {img_path}, {e}\")\n",
        "\n",
        "    return np.array(images), np.array(labels_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "zVxdzR_-bO3z"
      },
      "outputs": [],
      "source": [
        "def load_and_preprocess_data(subject_folders, labels):\n",
        "    train_subjects, test_subjects, train_labels, test_labels = train_test_split(subject_folders, labels, test_size=0.2, stratify=labels)\n",
        "\n",
        "    train_subjects, val_subjects, train_labels, val_labels = train_test_split(train_subjects, train_labels, test_size=0.1, stratify=train_labels)\n",
        "\n",
        "    train_images, train_labels_t = load_images_and_labels(train_subjects, train_labels)\n",
        "    val_images, val_labels_t = load_images_and_labels(val_subjects, val_labels)\n",
        "\n",
        "    test_image_counts_per_subject = [len(os.listdir(folder)) for folder in test_subjects]\n",
        "\n",
        "    test_images, test_labels_t = load_images_and_labels(test_subjects, test_labels)\n",
        "\n",
        "    return train_images, train_labels_t, val_images, val_labels_t, test_images, test_labels_t, test_subjects, test_image_counts_per_subject\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "sKYoz5C_cEij"
      },
      "outputs": [],
      "source": [
        "def create_new_model(base_model):\n",
        "    model = clone_model(base_model)\n",
        "\n",
        "    for layer in model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    x = model.output\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(256)(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(512)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=model.input, outputs=predictions)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "4MxuEguZcIX8"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_model(base_model, subject_folders, labels, num_epochs=50, num_iterations=5, model_name='model'):\n",
        "    accuracy_list, precision_list, recall_list, f1_list = [], [], [], []\n",
        "\n",
        "    for iteration in range(num_iterations):\n",
        "        print(f\"\\nIteration {iteration + 1}:\")\n",
        "\n",
        "        model = create_new_model(base_model)\n",
        "\n",
        "        train_images, train_labels_t, val_images, val_labels_t, test_images, test_labels_t, test_subjects, test_image_counts_per_subject = load_and_preprocess_data(subject_folders, labels)\n",
        "\n",
        "        train_data = list(zip(train_images, train_labels_t))\n",
        "        np.random.shuffle(train_data)\n",
        "        train_images, train_labels_t = zip(*train_data)\n",
        "\n",
        "        val_data = list(zip(val_images, val_labels_t))\n",
        "        np.random.shuffle(val_data)\n",
        "        val_images, val_labels_t = zip(*val_data)\n",
        "\n",
        "        # test_data = list(zip(test_images, test_labels_t))\n",
        "        # np.random.shuffle(test_data)\n",
        "        # test_images, test_labels_t = zip(*test_data)\n",
        "\n",
        "        train_images, train_labels_t = np.array(train_images), np.array(train_labels_t)\n",
        "        val_images, val_labels_t = np.array(val_images), np.array(val_labels_t)\n",
        "        # test_images, test_labels_t = np.array(test_images), np.array(test_labels_t)\n",
        "\n",
        "\n",
        "\n",
        "        # checkpoint_filepath = \"best_weights_iteration_{iteration + 1}.h5\"\n",
        "        # checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "        #                                        monitor='val_accuracy',\n",
        "        #                                        mode='max',\n",
        "        #                                        save_best_only=True,\n",
        "        #                                        save_weights_only=True,\n",
        "        #                                        verbose=1)\n",
        "\n",
        "        # history = model.fit(train_images, train_labels_t, epochs=num_epochs, batch_size=8, validation_data=(val_images, val_labels_t), callbacks=[checkpoint], verbose=1)\n",
        "\n",
        "        checkpoint_dir = 'F:/Abdipour/Spectrogram Images/'\n",
        "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "        # local_checkpoint_filepath = os.path.join(checkpoint_dir, f\"{model_name}_best_weights_iteration_{iteration + 1}.h5\")\n",
        "        # drive_checkpoint_filepath = f'/content/drive/My Drive/{model_name}_best_weights_iteration_{iteration + 1}.h5'\n",
        "        local_checkpoint_filepath = checkpoint_dir + '\\\\' + f\"{model_name}_best_weights_iteration_{iteration +1}.weights.h5\"\n",
        "\n",
        "        checkpoint_callback = ModelCheckpoint(filepath=local_checkpoint_filepath,\n",
        "                                               monitor='val_accuracy',\n",
        "                                               mode='max',\n",
        "                                               save_best_only=True,\n",
        "                                               save_weights_only=True,\n",
        "                                               verbose=1)\n",
        "\n",
        "        # class CustomEarlyStopping(EarlyStopping):\n",
        "        #     def on_epoch_end(self, epoch, logs=None):\n",
        "        #         if epoch > 30:\n",
        "        #             super().on_epoch_end(epoch - 30, logs)\n",
        "\n",
        "        class LearningRateLogger(Callback):\n",
        "            def on_epoch_end(self, epoch, logs=None):\n",
        "                logs = logs or {}\n",
        "                logs['learning_rate'] = self.model.optimizer.learning_rate.numpy()\n",
        "        lr_logger = LearningRateLogger()\n",
        "        # initial_epochs = 30        \n",
        "        # history = model.fit(train_images, train_labels_t, epochs=initial_epochs, batch_size=16, validation_data=(val_images, val_labels_t), verbose=1)\n",
        "\n",
        "        # early_stopping = CustomEarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', baseline=None, restore_best_weights=False, start_from_epoch=30)\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', baseline=None, restore_best_weights=False)\n",
        "        # history = model.fit(train_images, train_labels_t, epochs=num_epochs, batch_size=16, validation_data=(val_images, val_labels_t), callbacks=[checkpoint_callback, early_stopping], initial_epoch=initial_epochs, verbose=1)\n",
        "        history = model.fit(train_images, train_labels_t, epochs=num_epochs, batch_size=8, validation_data=(val_images, val_labels_t), callbacks=[checkpoint_callback, early_stopping], verbose=1)\n",
        "\n",
        "        # if os.path.exists(local_checkpoint_filepath):\n",
        "        #     print(f\"Best weights saved locally at: {local_checkpoint_filepath}\")\n",
        "\n",
        "        #     shutil.copy(local_checkpoint_filepath, drive_checkpoint_filepath)\n",
        "        #     if os.path.exists(drive_checkpoint_filepath):\n",
        "        #         print(f\"Best weights also copied to Google Drive at: {drive_checkpoint_filepath}\")\n",
        "        #     else:\n",
        "        #         print(f\"Failed to copy best weights to Google Drive at: {drive_checkpoint_filepath}\")\n",
        "        # else:\n",
        "        #     print(f\"Failed to save best weights locally at: {local_checkpoint_filepath}\")\n",
        "\n",
        "        model.load_weights(local_checkpoint_filepath)\n",
        "\n",
        "        test_predictions = model.predict(test_images)\n",
        "        test_predictions = (test_predictions > 0.5).astype(int)\n",
        "\n",
        "        accuracy = accuracy_score(test_labels_t, test_predictions)\n",
        "        precision = precision_score(test_labels_t, test_predictions)\n",
        "        recall = recall_score(test_labels_t, test_predictions)\n",
        "        f1 = f1_score(test_labels_t, test_predictions)\n",
        "\n",
        "        accuracy_list.append(accuracy)\n",
        "        precision_list.append(precision)\n",
        "        recall_list.append(recall)\n",
        "        f1_list.append(f1)\n",
        "\n",
        "        print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "        print(\"ModelCheckpoint callback triggered and saved the best weights.\")\n",
        "\n",
        "\n",
        "        plt.plot(history.history['accuracy'])\n",
        "        plt.plot(history.history['val_accuracy'])\n",
        "        plt.title('Model accuracy')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "        file_name = f'{model_name}_accuracy_iteration_{iteration + 1}.png'\n",
        "        plt.savefig(file_name)\n",
        "        plt.close()\n",
        "        # files.download(file_name)\n",
        "\n",
        "\n",
        "\n",
        "        plt.plot(history.history['loss'])\n",
        "        plt.plot(history.history['val_loss'])\n",
        "        plt.title('Model loss')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "        file_name = f'{model_name}_loss_iteration_{iteration + 1}.png'\n",
        "        plt.savefig(file_name)\n",
        "        plt.close()\n",
        "        # files.download(file_name)\n",
        "\n",
        "\n",
        "        cm = confusion_matrix(test_labels_t, test_predictions)\n",
        "        plt.figure(figsize=(6, 5))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', xticklabels=['AD', 'HC'], yticklabels=['AD', 'HC'])\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.title(f'Confusion Matrix - {model_name} - Iteration {iteration + 1}')\n",
        "        file_name = f'{model_name}_confusion_matrix_iteration_{iteration + 1}.png'\n",
        "        plt.savefig(file_name)\n",
        "        plt.close()\n",
        "        # files.download(file_name)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # print(\"\\nPredictions for Each Subject's Images:\")\n",
        "        # start_idx = 0\n",
        "        # for i, (subject_folder, num_images) in enumerate(zip(test_subjects, test_image_counts_per_subject)):\n",
        "        #     end_idx = start_idx + num_images\n",
        "        #     subject_images = test_images[start_idx:end_idx]\n",
        "        #     subject_predictions = (model.predict(subject_images) > 0.5).astype(int)\n",
        "        #     print(f\"Subject {i + 1} ({subject_folder}):\")\n",
        "        #     for j, prediction in enumerate(subject_predictions):\n",
        "        #         if prediction == 0:\n",
        "        #             print(f\"Image {j + 1}: AD\")\n",
        "        #         else:\n",
        "        #             print(f\"Image {j + 1}: HC\")\n",
        "        #     start_idx = end_idx\n",
        "\n",
        "\n",
        "        print(\"\\nPredictions for Each Subject's Images:\")\n",
        "        start_idx = 0\n",
        "        for i, (subject_folder, num_images) in enumerate(zip(test_subjects, test_image_counts_per_subject)):\n",
        "            end_idx = start_idx + num_images\n",
        "            subject_images = test_images[start_idx:end_idx]\n",
        "            subject_predictions = (model.predict(subject_images) > 0.5).astype(int)\n",
        "            ad_count = sum(subject_predictions == 0)\n",
        "            hc_count = sum(subject_predictions == 1)\n",
        "            print(f\"Subject {i + 1} ({subject_folder}):\")\n",
        "            print(f\"AD count: {ad_count}, HC count: {hc_count}\")\n",
        "            start_idx = end_idx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(\"\\nAverage Metrics:\")\n",
        "    print(f\"Average Accuracy: {np.mean(accuracy_list):.4f}\")\n",
        "    print(f\"Average Precision: {np.mean(precision_list):.4f}\")\n",
        "    print(f\"Average Recall: {np.mean(recall_list):.4f}\")\n",
        "    print(f\"Average F1 Score: {np.mean(f1_list):.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ImCoh "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "vgg16_base_model = VGG16(weights ='imagenet', include_top =False, input_shape = (224,224,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration 1:\n",
            "Epoch 1/100\n",
            "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 413ms/step - accuracy: 0.9934 - loss: 0.0491\n",
            "Epoch 1: val_accuracy improved from -inf to 1.00000, saving model to F:/Abdipour/Spectrogram Images/\\vgg16_imcoh_best_weights_iteration_1.weights.h5\n",
            "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 469ms/step - accuracy: 0.9934 - loss: 0.0489 - val_accuracy: 1.0000 - val_loss: 1.5575e-18\n",
            "Epoch 2/100\n",
            "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 1.0000 - loss: 3.7235e-16\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 474ms/step - accuracy: 1.0000 - loss: 3.7362e-16 - val_accuracy: 1.0000 - val_loss: 1.5575e-18\n",
            "Epoch 3/100\n",
            "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 1.0000 - loss: 1.5120e-15\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 474ms/step - accuracy: 1.0000 - loss: 1.5114e-15 - val_accuracy: 1.0000 - val_loss: 1.5575e-18\n",
            "Epoch 4/100\n",
            "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 1.0000 - loss: 5.0295e-16\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 475ms/step - accuracy: 1.0000 - loss: 5.0379e-16 - val_accuracy: 1.0000 - val_loss: 1.5575e-18\n",
            "Epoch 5/100\n",
            "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 1.0000 - loss: 7.7672e-16\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 473ms/step - accuracy: 1.0000 - loss: 7.7606e-16 - val_accuracy: 1.0000 - val_loss: 1.5575e-18\n",
            "Epoch 6/100\n",
            "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 407ms/step - accuracy: 1.0000 - loss: 8.4421e-16\n",
            "Epoch 6: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 460ms/step - accuracy: 1.0000 - loss: 8.4417e-16 - val_accuracy: 1.0000 - val_loss: 1.5575e-18\n",
            "Epoch 7/100\n",
            "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step - accuracy: 1.0000 - loss: 5.4792e-16\n",
            "Epoch 7: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 460ms/step - accuracy: 1.0000 - loss: 5.4842e-16 - val_accuracy: 1.0000 - val_loss: 1.5575e-18\n",
            "Epoch 8/100\n",
            "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step - accuracy: 1.0000 - loss: 1.1690e-15\n",
            "Epoch 8: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 461ms/step - accuracy: 1.0000 - loss: 1.1694e-15 - val_accuracy: 1.0000 - val_loss: 1.5575e-18\n",
            "Epoch 9/100\n",
            "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step - accuracy: 1.0000 - loss: 5.0279e-16\n",
            "Epoch 9: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 461ms/step - accuracy: 1.0000 - loss: 5.0382e-16 - val_accuracy: 1.0000 - val_loss: 1.5575e-18\n",
            "Epoch 10/100\n",
            "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 409ms/step - accuracy: 1.0000 - loss: 1.3122e-15\n",
            "Epoch 10: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 461ms/step - accuracy: 1.0000 - loss: 1.3104e-15 - val_accuracy: 1.0000 - val_loss: 1.5575e-18\n",
            "Epoch 11/100\n",
            "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step - accuracy: 1.0000 - loss: 1.4451e-15\n",
            "Epoch 11: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 461ms/step - accuracy: 1.0000 - loss: 1.4447e-15 - val_accuracy: 1.0000 - val_loss: 1.5575e-18\n",
            "Epoch 11: early stopping\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step\n",
            "Accuracy: 1.0000, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "ModelCheckpoint callback triggered and saved the best weights.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:395: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Predictions for Each Subject's Images:\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Subject 1 (F:/Abdipour/Connectivity/ImCoh images\\Subject3):\n",
            "AD count: [27], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 487ms/step\n",
            "Subject 2 (F:/Abdipour/Connectivity/ImCoh images\\Subject59):\n",
            "AD count: [42], HC count: [0]\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 952ms/step\n",
            "Subject 3 (F:/Abdipour/Connectivity/ImCoh images\\Subject15):\n",
            "AD count: [74], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step\n",
            "Subject 4 (F:/Abdipour/Connectivity/ImCoh images\\Subject53):\n",
            "AD count: [34], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 666ms/step\n",
            "Subject 5 (F:/Abdipour/Connectivity/ImCoh images\\Subject16):\n",
            "AD count: [46], HC count: [0]\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 933ms/step\n",
            "Subject 6 (F:/Abdipour/Connectivity/ImCoh images\\Subject10):\n",
            "AD count: [73], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Subject 7 (F:/Abdipour/Connectivity/ImCoh images\\Subject40):\n",
            "AD count: [31], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 863ms/step\n",
            "Subject 8 (F:/Abdipour/Connectivity/ImCoh images\\Subject54):\n",
            "AD count: [50], HC count: [0]\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 790ms/step\n",
            "Subject 9 (F:/Abdipour/Connectivity/ImCoh images\\Subject28):\n",
            "AD count: [67], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 258ms/step\n",
            "Subject 10 (F:/Abdipour/Connectivity/ImCoh images\\Subject27):\n",
            "AD count: [37], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 158ms/step\n",
            "Subject 11 (F:/Abdipour/Connectivity/ImCoh images\\Subject41):\n",
            "AD count: [35], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 212ms/step\n",
            "Subject 12 (F:/Abdipour/Connectivity/ImCoh images\\Subject11):\n",
            "AD count: [36], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Subject 13 (F:/Abdipour/Connectivity/ImCoh images\\Subject20):\n",
            "AD count: [30], HC count: [0]\n",
            "\n",
            "Average Metrics:\n",
            "Average Accuracy: 1.0000\n",
            "Average Precision: 0.0000\n",
            "Average Recall: 0.0000\n",
            "Average F1 Score: 0.0000\n"
          ]
        }
      ],
      "source": [
        "model_name = 'vgg16_imcoh'\n",
        "train_and_evaluate_model(vgg16_base_model, subject_folders, labels, num_epochs=100, num_iterations=1, model_name=model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration 1:\n",
            "Epoch 1/100\n",
            "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - accuracy: 0.9812 - loss: 0.0413\n",
            "Epoch 1: val_accuracy improved from -inf to 1.00000, saving model to F:/Abdipour/Spectrogram Images/\\vgg16_imcoh_best_weights_iteration_1.weights.h5\n",
            "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 476ms/step - accuracy: 0.9813 - loss: 0.0411 - val_accuracy: 1.0000 - val_loss: 2.3546e-22\n",
            "Epoch 2/100\n",
            "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - accuracy: 1.0000 - loss: 2.0776e-17\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 455ms/step - accuracy: 1.0000 - loss: 2.0739e-17 - val_accuracy: 1.0000 - val_loss: 2.3546e-22\n",
            "Epoch 3/100\n",
            "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 405ms/step - accuracy: 1.0000 - loss: 5.0874e-17\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 455ms/step - accuracy: 1.0000 - loss: 5.0749e-17 - val_accuracy: 1.0000 - val_loss: 2.3546e-22\n",
            "Epoch 4/100\n",
            "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - accuracy: 1.0000 - loss: 2.3991e-18\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 455ms/step - accuracy: 1.0000 - loss: 2.3968e-18 - val_accuracy: 1.0000 - val_loss: 2.3546e-22\n",
            "Epoch 5/100\n",
            "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 405ms/step - accuracy: 1.0000 - loss: 1.4932e-18\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 457ms/step - accuracy: 1.0000 - loss: 1.4936e-18 - val_accuracy: 1.0000 - val_loss: 2.3546e-22\n",
            "Epoch 6/100\n",
            "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step - accuracy: 1.0000 - loss: 5.5431e-19\n",
            "Epoch 6: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 459ms/step - accuracy: 1.0000 - loss: 5.5500e-19 - val_accuracy: 1.0000 - val_loss: 2.3546e-22\n",
            "Epoch 7/100\n",
            "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 407ms/step - accuracy: 1.0000 - loss: 3.2634e-18\n",
            "Epoch 7: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 458ms/step - accuracy: 1.0000 - loss: 3.2632e-18 - val_accuracy: 1.0000 - val_loss: 2.3546e-22\n",
            "Epoch 8/100\n",
            "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 407ms/step - accuracy: 1.0000 - loss: 1.8123e-18\n",
            "Epoch 8: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 457ms/step - accuracy: 1.0000 - loss: 1.8108e-18 - val_accuracy: 1.0000 - val_loss: 2.3546e-22\n",
            "Epoch 9/100\n",
            "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 407ms/step - accuracy: 1.0000 - loss: 7.8345e-18\n",
            "Epoch 9: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 458ms/step - accuracy: 1.0000 - loss: 7.8534e-18 - val_accuracy: 1.0000 - val_loss: 2.3546e-22\n",
            "Epoch 10/100\n",
            "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 409ms/step - accuracy: 1.0000 - loss: 4.5271e-18\n",
            "Epoch 10: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 463ms/step - accuracy: 1.0000 - loss: 4.5333e-18 - val_accuracy: 1.0000 - val_loss: 2.3546e-22\n",
            "Epoch 11/100\n",
            "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 412ms/step - accuracy: 1.0000 - loss: 1.2869e-18\n",
            "Epoch 11: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 463ms/step - accuracy: 1.0000 - loss: 1.3013e-18 - val_accuracy: 1.0000 - val_loss: 2.3546e-22\n",
            "Epoch 11: early stopping\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step\n",
            "Accuracy: 1.0000, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "ModelCheckpoint callback triggered and saved the best weights.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:395: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Predictions for Each Subject's Images:\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 202ms/step\n",
            "Subject 1 (F:/Abdipour/Connectivity/ImCoh images\\Subject5):\n",
            "AD count: [36], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Subject 2 (F:/Abdipour/Connectivity/ImCoh images\\Subject35):\n",
            "AD count: [22], HC count: [0]\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 752ms/step\n",
            "Subject 3 (F:/Abdipour/Connectivity/ImCoh images\\Subject36):\n",
            "AD count: [66], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Subject 4 (F:/Abdipour/Connectivity/ImCoh images\\Subject40):\n",
            "AD count: [31], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 908ms/step\n",
            "Subject 5 (F:/Abdipour/Connectivity/ImCoh images\\Subject25):\n",
            "AD count: [19], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 256ms/step\n",
            "Subject 6 (F:/Abdipour/Connectivity/ImCoh images\\Subject27):\n",
            "AD count: [37], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 721ms/step\n",
            "Subject 7 (F:/Abdipour/Connectivity/ImCoh images\\Subject14):\n",
            "AD count: [16], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Subject 8 (F:/Abdipour/Connectivity/ImCoh images\\Subject3):\n",
            "AD count: [27], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 843ms/step\n",
            "Subject 9 (F:/Abdipour/Connectivity/ImCoh images\\Subject54):\n",
            "AD count: [50], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step\n",
            "Subject 10 (F:/Abdipour/Connectivity/ImCoh images\\Subject2):\n",
            "AD count: [58], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step\n",
            "Subject 11 (F:/Abdipour/Connectivity/ImCoh images\\Subject31):\n",
            "AD count: [59], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Subject 12 (F:/Abdipour/Connectivity/ImCoh images\\Subject47):\n",
            "AD count: [23], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 629ms/step\n",
            "Subject 13 (F:/Abdipour/Connectivity/ImCoh images\\Subject50):\n",
            "AD count: [13], HC count: [0]\n",
            "\n",
            "Average Metrics:\n",
            "Average Accuracy: 1.0000\n",
            "Average Precision: 0.0000\n",
            "Average Recall: 0.0000\n",
            "Average F1 Score: 0.0000\n"
          ]
        }
      ],
      "source": [
        "model_name = 'vgg16_imcoh'\n",
        "train_and_evaluate_model(vgg16_base_model, subject_folders, labels, num_epochs=100, num_iterations=1, model_name=model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ResNet50\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "ResNet50_base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration 1:\n",
            "Epoch 1/100\n",
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 428ms/step - accuracy: 0.9829 - loss: 0.0326\n",
            "Epoch 1: val_accuracy improved from -inf to 1.00000, saving model to F:/Abdipour/Spectrogram Images/\\ResNet50_imcoh_best_weights_iteration_1.weights.h5\n",
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 477ms/step - accuracy: 0.9829 - loss: 0.0325 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 2/100\n",
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 461ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 3/100\n",
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 422ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 461ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 4/100\n",
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 461ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/100\n",
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 461ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/100\n",
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Epoch 6: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 460ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 7/100\n",
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Epoch 7: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 461ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 8/100\n",
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Epoch 8: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 498ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 9/100\n",
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Epoch 9: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 500ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 10/100\n",
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Epoch 10: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 498ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 11/100\n",
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 454ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Epoch 11: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 497ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 11: early stopping\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step\n",
            "Accuracy: 1.0000, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "ModelCheckpoint callback triggered and saved the best weights.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:395: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Predictions for Each Subject's Images:\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step\n",
            "Subject 1 (F:/Abdipour/Connectivity/ImCoh images\\Subject32):\n",
            "AD count: [57], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 437ms/step\n",
            "Subject 2 (F:/Abdipour/Connectivity/ImCoh images\\Subject52):\n",
            "AD count: [43], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Subject 3 (F:/Abdipour/Connectivity/ImCoh images\\Subject60):\n",
            "AD count: [29], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 652ms/step\n",
            "Subject 4 (F:/Abdipour/Connectivity/ImCoh images\\Subject14):\n",
            "AD count: [16], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 476ms/step\n",
            "Subject 5 (F:/Abdipour/Connectivity/ImCoh images\\Subject43):\n",
            "AD count: [44], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Subject 6 (F:/Abdipour/Connectivity/ImCoh images\\Subject29):\n",
            "AD count: [29], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 568ms/step\n",
            "Subject 7 (F:/Abdipour/Connectivity/ImCoh images\\Subject50):\n",
            "AD count: [13], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Subject 8 (F:/Abdipour/Connectivity/ImCoh images\\Subject20):\n",
            "AD count: [30], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 774ms/step\n",
            "Subject 9 (F:/Abdipour/Connectivity/ImCoh images\\Subject26):\n",
            "AD count: [19], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 550ms/step\n",
            "Subject 10 (F:/Abdipour/Connectivity/ImCoh images\\Subject55):\n",
            "AD count: [46], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Subject 11 (F:/Abdipour/Connectivity/ImCoh images\\Subject33):\n",
            "AD count: [29], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Subject 12 (F:/Abdipour/Connectivity/ImCoh images\\Subject38):\n",
            "AD count: [29], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step\n",
            "Subject 13 (F:/Abdipour/Connectivity/ImCoh images\\Subject21):\n",
            "AD count: [57], HC count: [0]\n",
            "\n",
            "Average Metrics:\n",
            "Average Accuracy: 1.0000\n",
            "Average Precision: 0.0000\n",
            "Average Recall: 0.0000\n",
            "Average F1 Score: 0.0000\n"
          ]
        }
      ],
      "source": [
        "model_name = 'ResNet50_imcoh'\n",
        "train_and_evaluate_model(ResNet50_base_model, subject_folders, labels, num_epochs=100, num_iterations=1, model_name=model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Densenet121\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "Dense_base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model_name = 'Densenet121_imcoh'\n",
        "train_and_evaluate_model(Dense_base_model, subject_folders, labels, num_epochs=100, num_iterations=5, model_name=model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pearson"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "vgg16_base_model = VGG16(weights ='imagenet', include_top =False, input_shape = (224,224,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration 1:\n",
            "Epoch 1/100\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - accuracy: 0.9872 - loss: 0.0405\n",
            "Epoch 1: val_accuracy improved from -inf to 1.00000, saving model to F:/Abdipour/Spectrogram Images/\\vgg16_pearson_best_weights_iteration_1.weights.h5\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 446ms/step - accuracy: 0.9872 - loss: 0.0403 - val_accuracy: 1.0000 - val_loss: 4.0210e-21\n",
            "Epoch 2/100\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 410ms/step - accuracy: 1.0000 - loss: 2.7513e-18\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 457ms/step - accuracy: 1.0000 - loss: 2.7575e-18 - val_accuracy: 1.0000 - val_loss: 4.0210e-21\n",
            "Epoch 3/100\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step - accuracy: 1.0000 - loss: 6.3875e-18\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 455ms/step - accuracy: 1.0000 - loss: 6.4001e-18 - val_accuracy: 1.0000 - val_loss: 4.0210e-21\n",
            "Epoch 4/100\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step - accuracy: 1.0000 - loss: 2.1670e-17\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 455ms/step - accuracy: 1.0000 - loss: 2.1741e-17 - val_accuracy: 1.0000 - val_loss: 4.0210e-21\n",
            "Epoch 5/100\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step - accuracy: 1.0000 - loss: 1.1024e-17\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 455ms/step - accuracy: 1.0000 - loss: 1.1008e-17 - val_accuracy: 1.0000 - val_loss: 4.0210e-21\n",
            "Epoch 6/100\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step - accuracy: 1.0000 - loss: 1.2004e-17\n",
            "Epoch 6: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 455ms/step - accuracy: 1.0000 - loss: 1.2016e-17 - val_accuracy: 1.0000 - val_loss: 4.0210e-21\n",
            "Epoch 7/100\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step - accuracy: 1.0000 - loss: 8.6908e-18\n",
            "Epoch 7: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 455ms/step - accuracy: 1.0000 - loss: 8.6788e-18 - val_accuracy: 1.0000 - val_loss: 4.0210e-21\n",
            "Epoch 8/100\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 407ms/step - accuracy: 1.0000 - loss: 1.0829e-17\n",
            "Epoch 8: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 455ms/step - accuracy: 1.0000 - loss: 1.0836e-17 - val_accuracy: 1.0000 - val_loss: 4.0210e-21\n",
            "Epoch 9/100\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step - accuracy: 1.0000 - loss: 7.8362e-17\n",
            "Epoch 9: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 455ms/step - accuracy: 1.0000 - loss: 7.8165e-17 - val_accuracy: 1.0000 - val_loss: 4.0210e-21\n",
            "Epoch 10/100\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 407ms/step - accuracy: 1.0000 - loss: 7.5566e-17\n",
            "Epoch 10: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 454ms/step - accuracy: 1.0000 - loss: 7.5384e-17 - val_accuracy: 1.0000 - val_loss: 4.0210e-21\n",
            "Epoch 11/100\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 407ms/step - accuracy: 1.0000 - loss: 2.3399e-17\n",
            "Epoch 11: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 454ms/step - accuracy: 1.0000 - loss: 2.3370e-17 - val_accuracy: 1.0000 - val_loss: 4.0210e-21\n",
            "Epoch 11: early stopping\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step\n",
            "Accuracy: 1.0000, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "ModelCheckpoint callback triggered and saved the best weights.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:395: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Predictions for Each Subject's Images:\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 838ms/step\n",
            "Subject 1 (F:/Abdipour/Connectivity/Pearson images\\Subject54):\n",
            "AD count: [50], HC count: [0]\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 750ms/step\n",
            "Subject 2 (F:/Abdipour/Connectivity/Pearson images\\Subject36):\n",
            "AD count: [66], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 706ms/step\n",
            "Subject 3 (F:/Abdipour/Connectivity/Pearson images\\Subject13):\n",
            "AD count: [48], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 892ms/step\n",
            "Subject 4 (F:/Abdipour/Connectivity/Pearson images\\Subject46):\n",
            "AD count: [51], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step\n",
            "Subject 5 (F:/Abdipour/Connectivity/Pearson images\\Subject57):\n",
            "AD count: [34], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 903ms/step\n",
            "Subject 6 (F:/Abdipour/Connectivity/Pearson images\\Subject25):\n",
            "AD count: [19], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Subject 7 (F:/Abdipour/Connectivity/Pearson images\\Subject47):\n",
            "AD count: [23], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 712ms/step\n",
            "Subject 8 (F:/Abdipour/Connectivity/Pearson images\\Subject63):\n",
            "AD count: [48], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 480ms/step\n",
            "Subject 9 (F:/Abdipour/Connectivity/Pearson images\\Subject59):\n",
            "AD count: [42], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Subject 10 (F:/Abdipour/Connectivity/Pearson images\\Subject3):\n",
            "AD count: [27], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 662ms/step\n",
            "Subject 11 (F:/Abdipour/Connectivity/Pearson images\\Subject16):\n",
            "AD count: [46], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Subject 12 (F:/Abdipour/Connectivity/Pearson images\\Subject45):\n",
            "AD count: [26], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 699ms/step\n",
            "Subject 13 (F:/Abdipour/Connectivity/Pearson images\\Subject39):\n",
            "AD count: [48], HC count: [0]\n",
            "\n",
            "Average Metrics:\n",
            "Average Accuracy: 1.0000\n",
            "Average Precision: 0.0000\n",
            "Average Recall: 0.0000\n",
            "Average F1 Score: 0.0000\n"
          ]
        }
      ],
      "source": [
        "model_name = 'vgg16_pearson'\n",
        "train_and_evaluate_model(vgg16_base_model, subject_folders, labels, num_epochs=100, num_iterations=1, model_name=model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ResNet50\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "ResNet50_base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration 1:\n",
            "Epoch 1/100\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455ms/step - accuracy: 1.0000 - loss: 0.0150"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResNet50_pearson\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mResNet50_base_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubject_folders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[33], line 69\u001b[0m, in \u001b[0;36mtrain_and_evaluate_model\u001b[1;34m(base_model, subject_folders, labels, num_epochs, num_iterations, model_name)\u001b[0m\n\u001b[0;32m     67\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, baseline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# history = model.fit(train_images, train_labels_t, epochs=num_epochs, batch_size=16, validation_data=(val_images, val_labels_t), callbacks=[checkpoint_callback, early_stopping], initial_epoch=initial_epochs, verbose=1)\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels_t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# if os.path.exists(local_checkpoint_filepath):\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m#     print(f\"Best weights saved locally at: {local_checkpoint_filepath}\")\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m#     print(f\"Failed to save best weights locally at: {local_checkpoint_filepath}\")\u001b[39;00m\n\u001b[0;32m     82\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(local_checkpoint_filepath)\n",
            "File \u001b[1;32mf:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mf:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:339\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_epoch_iterator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_epoch_iterator \u001b[38;5;241m=\u001b[39m TFEpochIterator(\n\u001b[0;32m    330\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m    331\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    337\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    338\u001b[0m     )\n\u001b[1;32m--> 339\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    351\u001b[0m }\n\u001b[0;32m    352\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
            "File \u001b[1;32mf:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mf:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:425\u001b[0m, in \u001b[0;36mTensorFlowTrainer.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    424\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m--> 425\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    426\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    427\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_end(step, logs)\n",
            "File \u001b[1;32mf:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mf:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mf:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:919\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m   \u001b[38;5;66;03m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[0;32m    914\u001b[0m   filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    915\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(\n\u001b[0;32m    916\u001b[0m           bound_args\n\u001b[0;32m    917\u001b[0m       )\n\u001b[0;32m    918\u001b[0m   )\n\u001b[1;32m--> 919\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    920\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn_with_cond\u001b[39m(inner_args, inner_kwds):\n\u001b[0;32m    925\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001b[39;00m\n",
            "File \u001b[1;32mf:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[1;32mf:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
            "File \u001b[1;32mf:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
            "File \u001b[1;32mf:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
            "File \u001b[1;32mf:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "model_name = 'ResNet50_pearson'\n",
        "train_and_evaluate_model(ResNet50_base_model, subject_folders, labels, num_epochs=100, num_iterations=1, model_name=model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Densenet121\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "Dense_base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model_name = 'Densenet121_pearson'\n",
        "train_and_evaluate_model(Dense_base_model, subject_folders, labels, num_epochs=100, num_iterations=5, model_name=model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsYyzttIh4X9"
      },
      "source": [
        "## InceptionV3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azaz_Mf_h2qJ",
        "outputId": "b1f5e388-95b5-4580-a26c-b5322c430944"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "inception_base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PSI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "vgg16_base_model = VGG16(weights ='imagenet', include_top =False, input_shape = (224,224,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration 1:\n",
            "Epoch 1/100\n",
            "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 419ms/step - accuracy: 0.9935 - loss: 0.0457\n",
            "Epoch 1: val_accuracy improved from -inf to 1.00000, saving model to F:/Abdipour/Spectrogram Images/\\vgg16_psi_best_weights_iteration_1.weights.h5\n",
            "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 470ms/step - accuracy: 0.9935 - loss: 0.0455 - val_accuracy: 1.0000 - val_loss: 2.2694e-19\n",
            "Epoch 2/100\n",
            "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415ms/step - accuracy: 1.0000 - loss: 1.0416e-16\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 465ms/step - accuracy: 1.0000 - loss: 1.0431e-16 - val_accuracy: 1.0000 - val_loss: 2.2694e-19\n",
            "Epoch 3/100\n",
            "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - accuracy: 1.0000 - loss: 6.6954e-17\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 471ms/step - accuracy: 1.0000 - loss: 6.7147e-17 - val_accuracy: 1.0000 - val_loss: 2.2694e-19\n",
            "Epoch 4/100\n",
            "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 425ms/step - accuracy: 1.0000 - loss: 6.4505e-17\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 476ms/step - accuracy: 1.0000 - loss: 6.4673e-17 - val_accuracy: 1.0000 - val_loss: 2.2694e-19\n",
            "Epoch 5/100\n",
            "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 427ms/step - accuracy: 1.0000 - loss: 9.3891e-17\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 478ms/step - accuracy: 1.0000 - loss: 9.3825e-17 - val_accuracy: 1.0000 - val_loss: 2.2694e-19\n",
            "Epoch 6/100\n",
            "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 422ms/step - accuracy: 1.0000 - loss: 1.0080e-16\n",
            "Epoch 6: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 473ms/step - accuracy: 1.0000 - loss: 1.0082e-16 - val_accuracy: 1.0000 - val_loss: 2.2694e-19\n",
            "Epoch 7/100\n",
            "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 1.0000 - loss: 2.0072e-16\n",
            "Epoch 7: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 471ms/step - accuracy: 1.0000 - loss: 2.0177e-16 - val_accuracy: 1.0000 - val_loss: 2.2694e-19\n",
            "Epoch 8/100\n",
            "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 428ms/step - accuracy: 1.0000 - loss: 1.3002e-16\n",
            "Epoch 8: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 478ms/step - accuracy: 1.0000 - loss: 1.3019e-16 - val_accuracy: 1.0000 - val_loss: 2.2694e-19\n",
            "Epoch 9/100\n",
            "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 434ms/step - accuracy: 1.0000 - loss: 6.4002e-17\n",
            "Epoch 9: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 486ms/step - accuracy: 1.0000 - loss: 6.4000e-17 - val_accuracy: 1.0000 - val_loss: 2.2694e-19\n",
            "Epoch 10/100\n",
            "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 431ms/step - accuracy: 1.0000 - loss: 1.2115e-16\n",
            "Epoch 10: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 482ms/step - accuracy: 1.0000 - loss: 1.2124e-16 - val_accuracy: 1.0000 - val_loss: 2.2694e-19\n",
            "Epoch 11/100\n",
            "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 431ms/step - accuracy: 1.0000 - loss: 4.4419e-17\n",
            "Epoch 11: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 482ms/step - accuracy: 1.0000 - loss: 4.4575e-17 - val_accuracy: 1.0000 - val_loss: 2.2694e-19\n",
            "Epoch 11: early stopping\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step\n",
            "Accuracy: 1.0000, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "ModelCheckpoint callback triggered and saved the best weights.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:395: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Predictions for Each Subject's Images:\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 821ms/step\n",
            "Subject 1 (F:/Abdipour/Connectivity/PSI images\\Subject49):\n",
            "AD count: [65], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 814ms/step\n",
            "Subject 2 (F:/Abdipour/Connectivity/PSI images\\Subject6):\n",
            "AD count: [47], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Subject 3 (F:/Abdipour/Connectivity/PSI images\\Subject48):\n",
            "AD count: [28], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 578ms/step\n",
            "Subject 4 (F:/Abdipour/Connectivity/PSI images\\Subject52):\n",
            "AD count: [43], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Subject 5 (F:/Abdipour/Connectivity/PSI images\\Subject35):\n",
            "AD count: [22], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step\n",
            "Subject 6 (F:/Abdipour/Connectivity/PSI images\\Subject56):\n",
            "AD count: [61], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 200ms/step\n",
            "Subject 7 (F:/Abdipour/Connectivity/PSI images\\Subject11):\n",
            "AD count: [36], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step\n",
            "Subject 8 (F:/Abdipour/Connectivity/PSI images\\Subject62):\n",
            "AD count: [34], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step\n",
            "Subject 9 (F:/Abdipour/Connectivity/PSI images\\Subject31):\n",
            "AD count: [59], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 907ms/step\n",
            "Subject 10 (F:/Abdipour/Connectivity/PSI images\\Subject25):\n",
            "AD count: [19], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Subject 11 (F:/Abdipour/Connectivity/PSI images\\Subject20):\n",
            "AD count: [30], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step\n",
            "Subject 12 (F:/Abdipour/Connectivity/PSI images\\Subject32):\n",
            "AD count: [57], HC count: [0]\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 781ms/step\n",
            "Subject 13 (F:/Abdipour/Connectivity/PSI images\\Subject36):\n",
            "AD count: [66], HC count: [0]\n",
            "\n",
            "Average Metrics:\n",
            "Average Accuracy: 1.0000\n",
            "Average Precision: 0.0000\n",
            "Average Recall: 0.0000\n",
            "Average F1 Score: 0.0000\n"
          ]
        }
      ],
      "source": [
        "model_name = 'vgg16_psi'\n",
        "train_and_evaluate_model(vgg16_base_model, subject_folders, labels, num_epochs=100, num_iterations=1, model_name=model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ResNet50\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "ResNet50_base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = 'ResNet50_psi'\n",
        "train_and_evaluate_model(ResNet50_base_model, subject_folders, labels, num_epochs=100, num_iterations=1, model_name=model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Densenet121\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "Dense_base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model_name = 'Densenet121_psi'\n",
        "train_and_evaluate_model(Dense_base_model, subject_folders, labels, num_epochs=100, num_iterations=5, model_name=model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AEC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "vgg16_base_model = VGG16(weights ='imagenet', include_top =False, input_shape = (224,224,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration 1:\n",
            "Epoch 1/100\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 419ms/step - accuracy: 0.9936 - loss: 0.0339\n",
            "Epoch 1: val_accuracy improved from -inf to 1.00000, saving model to F:/Abdipour/Spectrogram Images/\\vgg16_AEC_best_weights_iteration_1.weights.h5\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 483ms/step - accuracy: 0.9936 - loss: 0.0337 - val_accuracy: 1.0000 - val_loss: 2.0875e-25\n",
            "Epoch 2/100\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 413ms/step - accuracy: 1.0000 - loss: 1.0195e-20\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 472ms/step - accuracy: 1.0000 - loss: 1.0196e-20 - val_accuracy: 1.0000 - val_loss: 2.0875e-25\n",
            "Epoch 3/100\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - accuracy: 1.0000 - loss: 7.6590e-21\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 463ms/step - accuracy: 1.0000 - loss: 7.6757e-21 - val_accuracy: 1.0000 - val_loss: 2.0875e-25\n",
            "Epoch 4/100\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - accuracy: 1.0000 - loss: 3.6467e-20\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 464ms/step - accuracy: 1.0000 - loss: 3.6531e-20 - val_accuracy: 1.0000 - val_loss: 2.0875e-25\n",
            "Epoch 5/100\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - accuracy: 1.0000 - loss: 7.7193e-21\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 463ms/step - accuracy: 1.0000 - loss: 7.7149e-21 - val_accuracy: 1.0000 - val_loss: 2.0875e-25\n",
            "Epoch 6/100\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - accuracy: 1.0000 - loss: 4.7272e-21\n",
            "Epoch 6: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 463ms/step - accuracy: 1.0000 - loss: 4.7431e-21 - val_accuracy: 1.0000 - val_loss: 2.0875e-25\n",
            "Epoch 7/100\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - accuracy: 1.0000 - loss: 1.3740e-19\n",
            "Epoch 7: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 463ms/step - accuracy: 1.0000 - loss: 1.3921e-19 - val_accuracy: 1.0000 - val_loss: 2.0875e-25\n",
            "Epoch 8/100\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - accuracy: 1.0000 - loss: 1.9588e-20\n",
            "Epoch 8: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 463ms/step - accuracy: 1.0000 - loss: 1.9632e-20 - val_accuracy: 1.0000 - val_loss: 2.0875e-25\n",
            "Epoch 9/100\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - accuracy: 1.0000 - loss: 3.0788e-20\n",
            "Epoch 9: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 463ms/step - accuracy: 1.0000 - loss: 3.0783e-20 - val_accuracy: 1.0000 - val_loss: 2.0875e-25\n",
            "Epoch 10/100\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - accuracy: 1.0000 - loss: 4.1297e-20\n",
            "Epoch 10: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 463ms/step - accuracy: 1.0000 - loss: 4.1329e-20 - val_accuracy: 1.0000 - val_loss: 2.0875e-25\n",
            "Epoch 11/100\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - accuracy: 1.0000 - loss: 6.2294e-21\n",
            "Epoch 11: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 464ms/step - accuracy: 1.0000 - loss: 6.2350e-21 - val_accuracy: 1.0000 - val_loss: 2.0875e-25\n",
            "Epoch 11: early stopping\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step\n",
            "Accuracy: 1.0000, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "ModelCheckpoint callback triggered and saved the best weights.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:395: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Predictions for Each Subject's Images:\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 862ms/step\n",
            "Subject 1 (F:/Abdipour/Connectivity/PSI images\\Subject22):\n",
            "AD count: [19], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 633ms/step\n",
            "Subject 2 (F:/Abdipour/Connectivity/PSI images\\Subject55):\n",
            "AD count: [46], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 566ms/step\n",
            "Subject 3 (F:/Abdipour/Connectivity/PSI images\\Subject17):\n",
            "AD count: [44], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Subject 4 (F:/Abdipour/Connectivity/PSI images\\Subject3):\n",
            "AD count: [27], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 567ms/step\n",
            "Subject 5 (F:/Abdipour/Connectivity/PSI images\\Subject43):\n",
            "AD count: [44], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 198ms/step\n",
            "Subject 6 (F:/Abdipour/Connectivity/PSI images\\Subject5):\n",
            "AD count: [36], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Subject 7 (F:/Abdipour/Connectivity/PSI images\\Subject30):\n",
            "AD count: [30], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Subject 8 (F:/Abdipour/Connectivity/PSI images\\Subject45):\n",
            "AD count: [26], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Subject 9 (F:/Abdipour/Connectivity/PSI images\\Subject35):\n",
            "AD count: [22], HC count: [0]\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 940ms/step\n",
            "Subject 10 (F:/Abdipour/Connectivity/PSI images\\Subject15):\n",
            "AD count: [74], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 289ms/step\n",
            "Subject 11 (F:/Abdipour/Connectivity/PSI images\\Subject37):\n",
            "AD count: [38], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 154ms/step\n",
            "Subject 12 (F:/Abdipour/Connectivity/PSI images\\Subject19):\n",
            "AD count: [33], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Subject 13 (F:/Abdipour/Connectivity/PSI images\\Subject60):\n",
            "AD count: [29], HC count: [0]\n",
            "\n",
            "Average Metrics:\n",
            "Average Accuracy: 1.0000\n",
            "Average Precision: 0.0000\n",
            "Average Recall: 0.0000\n",
            "Average F1 Score: 0.0000\n"
          ]
        }
      ],
      "source": [
        "model_name = 'vgg16_AEC'\n",
        "train_and_evaluate_model(vgg16_base_model, subject_folders, labels, num_epochs=100, num_iterations=1, model_name=model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ResNet50\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "ResNet50_base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration 1:\n",
            "Epoch 1/100\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - accuracy: 0.9787 - loss: 0.0200\n",
            "Epoch 1: val_accuracy improved from -inf to 1.00000, saving model to F:/Abdipour/Spectrogram Images/\\ResNet50_AEC_best_weights_iteration_1.weights.h5\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 475ms/step - accuracy: 0.9788 - loss: 0.0199 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 2/100\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 422ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 464ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 3/100\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 465ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 4/100\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 465ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/100\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 466ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/100\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Epoch 6: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 465ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 7/100\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Epoch 7: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 466ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 8/100\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Epoch 8: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 463ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 9/100\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 422ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Epoch 9: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 463ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 10/100\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Epoch 10: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 465ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 11/100\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Epoch 11: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 466ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 11: early stopping\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step\n",
            "Accuracy: 1.0000, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "ModelCheckpoint callback triggered and saved the best weights.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "f:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:395: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Predictions for Each Subject's Images:\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 602ms/step\n",
            "Subject 1 (F:/Abdipour/Connectivity/PSI images\\Subject63):\n",
            "AD count: [48], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 594ms/step\n",
            "Subject 2 (F:/Abdipour/Connectivity/PSI images\\Subject7):\n",
            "AD count: [48], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 765ms/step\n",
            "Subject 3 (F:/Abdipour/Connectivity/PSI images\\Subject8):\n",
            "AD count: [20], HC count: [0]\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 633ms/step\n",
            "Subject 4 (F:/Abdipour/Connectivity/PSI images\\Subject36):\n",
            "AD count: [66], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 950ms/step\n",
            "Subject 5 (F:/Abdipour/Connectivity/PSI images\\Subject21):\n",
            "AD count: [57], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Subject 6 (F:/Abdipour/Connectivity/PSI images\\Subject33):\n",
            "AD count: [29], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Subject 7 (F:/Abdipour/Connectivity/PSI images\\Subject30):\n",
            "AD count: [30], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step\n",
            "Subject 8 (F:/Abdipour/Connectivity/PSI images\\Subject53):\n",
            "AD count: [34], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 516ms/step\n",
            "Subject 9 (F:/Abdipour/Connectivity/PSI images\\Subject55):\n",
            "AD count: [46], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 943ms/step\n",
            "Subject 10 (F:/Abdipour/Connectivity/PSI images\\Subject32):\n",
            "AD count: [57], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 412ms/step\n",
            "Subject 11 (F:/Abdipour/Connectivity/PSI images\\Subject52):\n",
            "AD count: [43], HC count: [0]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 737ms/step\n",
            "Subject 12 (F:/Abdipour/Connectivity/PSI images\\Subject26):\n",
            "AD count: [19], HC count: [0]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step\n",
            "Subject 13 (F:/Abdipour/Connectivity/PSI images\\Subject64):\n",
            "AD count: [59], HC count: [0]\n",
            "\n",
            "Average Metrics:\n",
            "Average Accuracy: 1.0000\n",
            "Average Precision: 0.0000\n",
            "Average Recall: 0.0000\n",
            "Average F1 Score: 0.0000\n"
          ]
        }
      ],
      "source": [
        "model_name = 'ResNet50_AEC'\n",
        "train_and_evaluate_model(ResNet50_base_model, subject_folders, labels, num_epochs=100, num_iterations=1, model_name=model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Densenet121\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n"
          ]
        },
        {
          "ename": "Exception",
          "evalue": "URL fetch failure on https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5: 403 -- Forbidden",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[1;32mf:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\keras\\src\\utils\\file_utils.py:291\u001b[0m, in \u001b[0;36mget_file\u001b[1;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir, force_download)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 291\u001b[0m     \u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDLProgbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:241\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    239\u001b[0m url_type, path \u001b[38;5;241m=\u001b[39m _splittype(url)\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mclosing(\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    242\u001b[0m     headers \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39minfo()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    562\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 563\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    495\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
            "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Dense_base_model \u001b[38;5;241m=\u001b[39m \u001b[43mDenseNet121\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimagenet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_top\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mf:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\keras\\src\\applications\\densenet.py:342\u001b[0m, in \u001b[0;36mDenseNet121\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\n\u001b[0;32m    327\u001b[0m     [\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.applications.densenet.DenseNet121\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    339\u001b[0m     classifier_activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    340\u001b[0m ):\n\u001b[0;32m    341\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Instantiates the Densenet121 architecture.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDenseNet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_top\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpooling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclassifier_activation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mf:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\keras\\src\\applications\\densenet.py:299\u001b[0m, in \u001b[0;36mDenseNet\u001b[1;34m(blocks, include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m blocks \u001b[38;5;241m==\u001b[39m [\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m24\u001b[39m, \u001b[38;5;241m16\u001b[39m]:\n\u001b[1;32m--> 299\u001b[0m         weights_path \u001b[38;5;241m=\u001b[39m \u001b[43mfile_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdensenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m            \u001b[49m\u001b[43mDENSENET121_WEIGHT_PATH_NO_TOP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_subdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfile_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m30ee3e1110167f948a6b9946edeeb738\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m blocks \u001b[38;5;241m==\u001b[39m [\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m]:\n\u001b[0;32m    306\u001b[0m         weights_path \u001b[38;5;241m=\u001b[39m file_utils\u001b[38;5;241m.\u001b[39mget_file(\n\u001b[0;32m    307\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdensenet169_weights_tf_dim_ordering_tf_kernels_notop.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    308\u001b[0m             DENSENET169_WEIGHT_PATH_NO_TOP,\n\u001b[0;32m    309\u001b[0m             cache_subdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    310\u001b[0m             file_hash\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb8c4d4c20dd625c148057b9ff1c1176b\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    311\u001b[0m         )\n",
            "File \u001b[1;32mf:\\Abdipour\\pythonProject\\venv\\lib\\site-packages\\keras\\src\\utils\\file_utils.py:293\u001b[0m, in \u001b[0;36mget_file\u001b[1;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir, force_download)\u001b[0m\n\u001b[0;32m    291\u001b[0m     urlretrieve(origin, fpath, DLProgbar())\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(error_msg\u001b[38;5;241m.\u001b[39mformat(origin, e\u001b[38;5;241m.\u001b[39mcode, e\u001b[38;5;241m.\u001b[39mmsg))\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(error_msg\u001b[38;5;241m.\u001b[39mformat(origin, e\u001b[38;5;241m.\u001b[39merrno, e\u001b[38;5;241m.\u001b[39mreason))\n",
            "\u001b[1;31mException\u001b[0m: URL fetch failure on https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5: 403 -- Forbidden"
          ]
        }
      ],
      "source": [
        "Dense_base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = 'Densenet121_AEC'\n",
        "train_and_evaluate_model(Dense_base_model, subject_folders, labels, num_epochs=100, num_iterations=5, model_name=model_name)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
